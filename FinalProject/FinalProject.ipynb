{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Course Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Guan GUAN | GGS8662\n",
    "<br><b>Jingyao YU | JYB4658\n",
    "<br><b>Lingfei CUI | LCY6897"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Background</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nowadays, people download Android Apps from Google Play and IOS Apps from Apple Store. In the Google Play, people feel it’s difficult to find the best one among those similar Apps especially for some new ones. It is impossible for them to use each of them and find the most usable one. So a more precisely rating is great and convenient to give users a reference when choose a functional App. An accurate rate could save time and give people better experience. Also it could help developers to improve their product.\n",
    "\n",
    "<br>Our task is helpful that people could get basic idea about an app that they are not familiar with. They could basically know which app is most likely good among their multiple choices. Meanwhile, developers could use this rating as a reference of prediction before releasing updated version. For example, they may have an idea of a more popular name of their app."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Dataset and Methodologies</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we investigated and predicted the rating of Apps based on attributes of the 12 terms, includes App, Category, Reviews, Size, Installs, Type, Price, Content Rating, Genres, Last Updated, Current Ver and Android Ver. We scratch data from www.kaggle.com and use the decision tree and LOOCV to get our preliminary results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self):\n",
    "        self.label = None\n",
    "        self.children = {}\n",
    "        self.saveChildren={}\n",
    "        self.isLeaf = True\n",
    "        self.attribute = ''\n",
    "        # you may want to add additional fields here...\n",
    "\n",
    "    def __init__(self, attribute, children, label, isLeaf=False):\n",
    "        self.attribute = attribute\n",
    "        self.children = children\n",
    "        self.label = label\n",
    "        self.isLeaf = isLeaf\n",
    "\n",
    "\n",
    "    def get_label(self):\n",
    "        return self.label\n",
    "\n",
    "    def get_children(self):\n",
    "        return self.children\n",
    "\n",
    "    def get_child(self, val):\n",
    "        if self.children.get(val):\n",
    "            return self.children[val]\n",
    "        return None\n",
    "\n",
    "    def get_atrribute(self):\n",
    "        return self.attribute\n",
    "\n",
    "    def checkLeaf(self):\n",
    "        return self.isLeaf\n",
    "\n",
    "    def set_to_leaf(self,label):\n",
    "        self.saveChildren=self.children\n",
    "        self.children={}\n",
    "        self.isLeaf=True\n",
    "        self.label=label\n",
    "\n",
    "    def recoveprune(self):\n",
    "        self.children = self.saveChildren\n",
    "        self.saveChildren = {}\n",
    "        self.isLeaf = False\n",
    "        self.label = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we used the decision tree so the first and probably the trickiest work we need to do is to design our tree nodes. We found it is hard to decide what features need to be used to best describe the node, so firstly followed the basic logic of a tree which is a tree could not exist without children, we decided to include children to clarify the child node of the root node; then we knew that we could get the final classification at the leaf nodes of our tree, the feature isLeaf is used as the symbol as the leaf nodes. Together with the isLeaf feature, the classification of the input app could be presented by leaf nodes’ label, then label feature is added as part of the design of the tree node; at last, the process of the classification is the process of examining whether the input app satisfying various attributes, then the attribute of the node is also important as it is the judging criteria of the input app, therefore attribute is also part of the features of the tree nodes.\n",
    "\n",
    "<br>Above the discussion, the node for our tree is fully designed. There are four features in the node, which are attribute, children, label and isLeaf; attribute describes what the node it is, since the raw data that we used maintains 13 attributes, we need to clarify the attribute of every node; children is just the normal children for a tree which describes the child nodes of the tree; label feature only work when you reach the leaf nodes which indicates the final category of the input app; and isLeaf acts like the notice to tell whether the node is a leaf node, and this feature should work with the label feature together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import copy\n",
    "import math\n",
    "\n",
    "def ID3(examples, default):\n",
    "    '''\n",
    "    Takes in an array of examples, and returns a tree (an instance of Node)\n",
    "    trained on the examples.  Each example is a dictionary of attribute:value pairs,\n",
    "    and the target class variable is a special attribute with the name \"Class\".\n",
    "    Any missing attributes are denoted with a value of \"?\"\n",
    "    '''\n",
    "    completeData = missingProcess(examples)\n",
    "    classify_values = [data['Class'] for data in completeData]\n",
    "\n",
    "    if len(completeData) == 0:\n",
    "        return Node('Class', {}, default, True)\n",
    "\n",
    "    # only have one classfication group\n",
    "    elif classify_values.count(classify_values[0]) == len(classify_values):\n",
    "        return Node('Class', {}, classify_values[0], True)\n",
    "\n",
    "    # only have one attribute\n",
    "    elif len(completeData[0]) == 1:\n",
    "        return Node('Class', {}, mostCommon(completeData, 'Class'), True)\n",
    "\n",
    "    else:\n",
    "        # this is the best attribute, attribute can not be the 'Class'\n",
    "        best = chooseAttr(completeData)  \n",
    "        root = Node(best, {}, None, False)\n",
    "\n",
    "        for value in bestAttrValues(completeData, best):\n",
    "            newData = getExamplesWithValue(completeData, best, value)\n",
    "            subtree = ID3(newData, mostCommon(newData, 'Class'))\n",
    "            root.children[value] = subtree\n",
    "    return root\n",
    "\n",
    "\n",
    "# according to the gainfeature, find attr which has the maximum gainfeature\n",
    "def chooseAttr(examples):\n",
    "    max = 0\n",
    "    max_attr = ''\n",
    "    for key in examples[0].keys():\n",
    "\n",
    "        if key != 'Class':\n",
    "            gain = gainfeature(examples, key)\n",
    "            if max <= gain:\n",
    "                max = gain\n",
    "                max_attr = key\n",
    "\n",
    "    return max_attr\n",
    "\n",
    "\n",
    "def test(node, examples):\n",
    "    '''\n",
    "    Takes in a trained tree and a test set of examples.  Returns the accuracy (fraction\n",
    "    of examples the tree classifies correctly).\n",
    "    '''\n",
    "    examples = missingProcess(examples)\n",
    "    correct = 0.0\n",
    "    start = (int)(len(examples) / 10 * 9)\n",
    "    end = len(examples) - 1\n",
    "    for i in examples[start : end]:\n",
    "        if evaluate(node, i) == i['Class']:\n",
    "            correct += 1.0\n",
    "\n",
    "    result = correct / (end - start + 1)\n",
    "    return result\n",
    "\n",
    "def evaluate(node, example):\n",
    "    '''\n",
    "    Takes in a tree and one example.  Returns the Class value that the tree\n",
    "    assigns to the example.\n",
    "    '''\n",
    "    if node == None:\n",
    "        return example['Class']\n",
    "    if node.get_children() == {}:\n",
    "        result = node.get_label()\n",
    "        return result\n",
    "    return evaluate(node.get_child(example[node.get_atrribute()]), example)\n",
    "\n",
    "\n",
    "def missingProcess(examples):\n",
    "    '''\n",
    "    For each attribute, find the most common value for this\n",
    "    :param example: [dict(a=1, b=0, Class=1),\n",
    "                     dict(a=1, b=0, Class=1)\n",
    "                     dict(a=0, b=1, Class=1)]\n",
    "    :return: dict(a=1, b=0)\n",
    "    '''\n",
    "    commonDic = {}\n",
    "    for key in examples[0].keys():\n",
    "        if key != \"Class\":\n",
    "            commonDic[key] = mostCommon(examples, key)\n",
    "\n",
    "    # update the examples again\n",
    "    for example in examples:\n",
    "        for key in example.keys():\n",
    "            if example[key] == 'NaN':\n",
    "                example[key] = commonDic[key]\n",
    "\n",
    "    return examples\n",
    "\n",
    "\n",
    "def mostCommon(examples, key):\n",
    "    valueDic = {}\n",
    "    values = [example[key] for example in examples]\n",
    "    # for each key, make a dict to store all their value and its frequence\n",
    "    for v in values:\n",
    "        if v in valueDic:\n",
    "            valueDic[v] += 1.0\n",
    "        else:\n",
    "            valueDic[v] = 1.0\n",
    "\n",
    "    # return the max frequence's value\n",
    "    max_value = ''\n",
    "    max = 0.0\n",
    "    for value in valueDic.keys():\n",
    "        if max < valueDic.get(value):\n",
    "            max = valueDic[value]\n",
    "            max_value = value\n",
    "    return max_value\n",
    "\n",
    "\n",
    "def entropy(examples):\n",
    "    data = [example['Class'] for example in examples]\n",
    "    index = {}\n",
    "    for i in data:\n",
    "        if i not in index:\n",
    "            index[i] = 1\n",
    "        else:\n",
    "            index[i] += 1\n",
    "    result = 0.0\n",
    "    for j in index:\n",
    "        result += index[j] / len(data) * (math.log2(index[j] / len(data)))\n",
    "    return result * -1\n",
    "\n",
    "def gainfeature(examples,attri):\n",
    "    data = [example[attri] for example in examples]\n",
    "    index = {}\n",
    "    for i in data:\n",
    "        if i not in index:\n",
    "            index[i] = 1\n",
    "        else:\n",
    "            index[i] += 1\n",
    "    result = 0.0\n",
    "    for j in index:\n",
    "        testexample = [example for example in examples if example[attri] == j]\n",
    "        result += index[j] / len(data) * entropy(testexample)\n",
    "    result = entropy(examples) - result\n",
    "    return result\n",
    "\n",
    "def getExamplesWithValue(data, attribute, value):\n",
    "    '''\n",
    "    Takes in a complete set of examples, the target attribute and the value\n",
    "    find all examples that the attribute' field is equal to value\n",
    "    :param data:\n",
    "    :param attribute:\n",
    "    :param value:\n",
    "    :return: array of dictionary\n",
    "    '''\n",
    "    res = []\n",
    "    for example in data:\n",
    "        if example[attribute] == value:\n",
    "            newExample = {}\n",
    "            for key, val in example.items():\n",
    "                if key != attribute:\n",
    "                    newExample[key] = val\n",
    "            res.append(newExample)\n",
    "    return res\n",
    "\n",
    "# find all different values of attribute in examples\n",
    "def bestAttrValues(examples, attribute):\n",
    "    '''\n",
    "    Takes in a complete set of examples and the target attribute, find all the unique\n",
    "    values in the attribute\n",
    "    :param examples: list of dictionary\n",
    "    :param attribute:\n",
    "    :return: a list of values\n",
    "    '''\n",
    "    res = []\n",
    "    for data in examples:\n",
    "        if data[attribute] not in res:\n",
    "            res.append(data[attribute])\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def prune(node, examples):\n",
    "    '''\n",
    "    Takes in a trained tree and a validation set of examples.  Prunes nodes in order\n",
    "    to improve accuracy on the validation data; the precise pruning strategy is up to you.\n",
    "    '''\n",
    "    return pruneWithroot(node, examples, node)\n",
    "\n",
    "def pruneWithroot(node, examples, root):\n",
    "    if node.isLeaf == True:\n",
    "        return node\n",
    "\n",
    "    if isChildAllLeaf(node):\n",
    "        currentScore = test(root, examples)\n",
    "        label = mostFreLabel(node)\n",
    "        node.set_to_leaf(label)\n",
    "        AfterScore = test(root, examples)\n",
    "        if currentScore > AfterScore:\n",
    "            node.recoveprune()\n",
    "        return node\n",
    "    else:\n",
    "        for child in node.children:\n",
    "            evaluateNode = pruneWithroot(node.children[child], examples, root)\n",
    "            node.children[child] = evaluateNode\n",
    "        return node\n",
    "\n",
    "def isChildAllLeaf(node):\n",
    "    '''\n",
    "    for the given node, check the node position, is the position of node could be pruned?\n",
    "    :param node:\n",
    "    :return: True or False\n",
    "    '''\n",
    "    test_children = node.get_children()\n",
    "    for child in test_children.values():\n",
    "        if not child.isLeaf:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def mostFreLabel(node):\n",
    "    labelList = [child.get_label() for child in node.get_children().values()]\n",
    "    mostfrelabel = Counter(labelList).most_common(1)\n",
    "    return mostfrelabel[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used ID3 algorithm to do the decision tree learning, but before the algorithm starts to work, we did the data pre-processing since there are some data losses in the raw data from Kaggle, here you can see that we firstly input our raw data into the missingProcess function, and basically the usage of this function is to make up the loss attribute with the attribute which repeated most often. Then we went through the three basic corner cases, which takes the bounding cases into consideration. By using the function called chooseAttr(choose attribute), we could set the attribute of every root node according to the value of the gainfeature, and basically the gainfeature is computed by calculating the entropy and then the information gain of every attribute, the function selects the attribute which maintains the smallest entropy (or we can say the largest information gain) step by step in each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import codecs\n",
    "\n",
    "def parse(filename):\n",
    "    '''\n",
    "    takes a filename and returns attribute information and all the data in array of dictionaries\n",
    "    '''\n",
    "    # initialize variables\n",
    "    out = []\n",
    "    csvfile = open(filename,'r',encoding='UTF-8')\n",
    "    fileToRead = csv.reader(csvfile)\n",
    "    \n",
    "    headers = next(fileToRead)\n",
    "    length=len(headers)\n",
    "    for i in range(length):\n",
    "        if headers[i]=='Rating':\n",
    "            headers[i]='Class'\n",
    "            \n",
    "    # iterate through rows of actual data\n",
    "    for row in fileToRead:\n",
    "        if len(row) == len(headers) and row[2] != 'NaN':\n",
    "            out.append(dict(zip(headers, row)))\n",
    "            \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse function in here is used to change the format of the raw data from Kaggle to the into “UTF-8” format which could be used as our training and testing data. And unit_test function maintains the testing process of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def testID3AndEvaluate():\n",
    "    data = [dict(a=1, b=0, Class=1), dict(a=1, b=1, Class=1)]\n",
    "    tree = ID3(data, 0)\n",
    "    if tree != None:\n",
    "        ans = evaluate(tree, dict(a=1, b=0))\n",
    "        if ans != 1:\n",
    "            print(\"ID3 test failed.\")\n",
    "        else:\n",
    "            print(\"ID3 test succeeded.\")\n",
    "    else:\n",
    "        print(\"ID3 test failed -- no tree returned\")\n",
    "\n",
    "def testPruning():\n",
    "    data = [dict(a=0, b=1, c=1, d=0, Class=1), dict(a=0, b=0, c=1, d=0, Class=0), dict(a=0, b=1, c=0, d=0, Class=1), dict(a=1, b=0, c=1, d=0, Class=0), dict(a=1, b=1, c=0, d=0, Class=0), dict(a=1, b=1, c=0, d=1, Class=0), dict(a=1, b=1, c=1, d=0, Class=0)]\n",
    "    validationData = [dict(a=0, b=0, c=1, d=0, Class=1), dict(a=1, b=1, c=1, d=1, Class = 0)]\n",
    "    tree = ID3(data, 0)\n",
    "    prune(tree, validationData)\n",
    "    if tree != None:\n",
    "        ans = evaluate(tree, dict(a=0, b=0, c=1, d=0))\n",
    "        if ans != 1:\n",
    "            print(\"pruning test failed.\")\n",
    "        else:\n",
    "            print(\"pruning test succeeded.\")\n",
    "    else:\n",
    "        print(\"pruning test failed -- no tree returned.\")\n",
    "\n",
    "\n",
    "def testID3AndTest():\n",
    "    trainData = [dict(a=1, b=0, c=0, Class=1), dict(a=1, b=1, c=0, Class=1), \n",
    "    dict(a=0, b=0, c=0, Class=0), dict(a=0, b=1, c=0, Class=1)]\n",
    "    testData = [dict(a=1, b=0, c=1, Class=1), dict(a=1, b=1, c=1, Class=1), \n",
    "    dict(a=0, b=0, c=1, Class=0), dict(a=0, b=1, c=1, Class=0)]\n",
    "    tree = ID3(trainData, 0)\n",
    "    fails = 0\n",
    "    if tree != None:\n",
    "        acc = test(tree, trainData)\n",
    "        if acc == 1.0:\n",
    "            print(\"testing on train data succeeded.\")\n",
    "        else:\n",
    "            print(\"testing on train data failed.\")\n",
    "            fails = fails + 1\n",
    "        acc = test(tree, testData)\n",
    "        if acc == 0.75:\n",
    "            print(\"testing on test data succeeded.\")\n",
    "        else:\n",
    "            print(\"testing on test data failed.\")\n",
    "            fails = fails + 1\n",
    "        if fails > 0:\n",
    "            print(\"Failures: \", fails)\n",
    "        else:\n",
    "            print(\"testID3AndTest succeeded.\")\n",
    "    else:\n",
    "        print(\"testID3andTest failed -- no tree returned.\")\t\n",
    "\n",
    "# inFile - string location of the house data file\n",
    "def testPruningOnHouseData(inFile):\n",
    "    withPruning = []\n",
    "    withoutPruning = []\n",
    "    data = parse(inFile)\n",
    "    for i in range(100):\n",
    "        random.shuffle(data)\n",
    "        train = data[: 9 * len(data)//10]\n",
    "        test = data[9 * len(data)//10:]\n",
    "\n",
    "        tree = ID3(train, 'democrat')\n",
    "        acc = test(tree, train)\n",
    "        print(\"training accuracy: \",acc)\n",
    "        acc = test(tree, test)\n",
    "        print(\"test accuracy: \",acc)\n",
    "  \n",
    "        acc = test(tree, test)\n",
    "        print(\"pruned tree test accuracy: \",acc)\n",
    "        withPruning.append(acc)\n",
    "\n",
    "        acc = test(tree, test)\n",
    "        print(\"no pruning test accuracy: \",acc)\n",
    "        withoutPruning.append(acc)\n",
    "    print(withPruning)\n",
    "    print(withoutPruning)\n",
    "    print(\"average with pruning\",sum(withPruning)/len(withPruning),\" without: \",sum(withoutPruning)/len(withoutPruning))\n",
    "\n",
    "def SimilarOnHouseData(data, trainSize):\n",
    "    res=[]\n",
    "    withPruning = []\n",
    "    withoutPruning = []\n",
    "    trainPartSize=int(trainSize/2)\n",
    "    validSize=int(3*trainSize/4)\n",
    "    for i in range(100):\n",
    "        random.shuffle(data)\n",
    "        train = data[:trainPartSize]\n",
    "        valid = data[trainPartSize:validSize]\n",
    "        test = data[validSize:]\n",
    "\n",
    "        tree = ID3(train, 'democrat')\n",
    "        acc = test(tree, train)\n",
    "        acc = test(tree, valid)\n",
    "        acc = test(tree, test)\n",
    "\n",
    "        prune(tree, valid)\n",
    "        acc = test(tree, train)\n",
    "        acc = test(tree, valid)\n",
    "        acc = test(tree, test)\n",
    "        withPruning.append(acc)\n",
    "\n",
    "        tree = ID3(train + valid, 'democrat')\n",
    "        acc = test(tree, test)\n",
    "        withoutPruning.append(acc)\n",
    "    print(\"average with pruning\", sum(withPruning) / len(withPruning), \" without: \",sum(withoutPruning) / len(withoutPruning))\n",
    "    res.append(sum(withPruning) / len(withPruning))\n",
    "    res.append(sum(withoutPruning) / len(withoutPruning))\n",
    "    return res\n",
    "\n",
    "def graphtest(infile):\n",
    "    data = parse(infile)\n",
    "    pruneRes=[]\n",
    "    WithoutPruneRes=[]\n",
    "    for trainSize in range(10,301,10):\n",
    "        result=SimilarOnHouseData(data,trainSize)\n",
    "        pruneRes.append(result[0])\n",
    "        WithoutPruneRes.append(result[1])\n",
    "\n",
    "    x_axis=np.arange(10,301,10)\n",
    "\n",
    "    axes = plt.gca()\n",
    "    axes.set_ylim([0.6, 1])\n",
    "\n",
    "    plt.figure(1, dpi=50)\n",
    "    plt.plot(x_axis, pruneRes, color='r',marker='.')\n",
    "    plt.plot(x_axis, WithoutPruneRes, color='b', marker='.')\n",
    "    labels=['average with pruning', 'average without pruning']\n",
    "    plt.legend(labels,loc=4)\n",
    "    plt.title('average accuracy between different training sizes')\n",
    "    plt.xlabel('number of training examples')\n",
    "    plt.ylabel('accuracy on test data')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def mini_grader():\n",
    "\n",
    "    data = [dict(a=1, b=0, Class=1), dict(a=1, b=1, Class=1)]\n",
    "\n",
    "    try:\n",
    "        tree = ID3(data, 0)\n",
    "        if tree != None:\n",
    "            ans = evaluate(tree, dict(a=1, b=0))\n",
    "            if ans != 1:\n",
    "                print(\"ID3 test 1 failed.\")\n",
    "            else:\n",
    "                print(\"ID3 test 1 succeeded.\")\n",
    "        else:\n",
    "            print(\"ID3 test 1 failed -- no tree returned\")\n",
    "    except Exception:\n",
    "        print(\"ID3 test 1 failed runtime error\")\n",
    "\n",
    "    data = [dict(a=1, b=0, Class=0), dict(a=1, b=1, Class=1)]\n",
    "\n",
    "    try:\n",
    "        tree = ID3(data, 0)\n",
    "        if tree != None:\n",
    "            ans = evaluate(tree, dict(a=1, b=0))\n",
    "            if ans != 0:\n",
    "                print(\"ID3 test 2 failed.\")\n",
    "            else:\n",
    "                print(\"ID3 test 2 succeeded.\")\n",
    "        else:\n",
    "            print(\"ID3 test 2 failed -- no tree returned\")\n",
    "    except Exception:\n",
    "        print(\"ID3 test 2 failed runtime error\")\n",
    "\n",
    "    data = [dict(a=1, b=0, Class=2), dict(a=1, b=1, Class=1),\n",
    "            dict(a=2, b=0, Class=2), dict(a=2, b=1, Class=3),\n",
    "            dict(a=3, b=0, Class=1), dict(a=3, b=1, Class=3)]\n",
    "\n",
    "    try:\n",
    "        tree = ID3(data, 0)\n",
    "        if tree != None:\n",
    "            ans = evaluate(tree, dict(a=1, b=0))\n",
    "            if ans != 2:\n",
    "                print(\"ID3 test 3-1 failed.\")\n",
    "            else:\n",
    "                print(\"ID3 test 3-1 succeeded.\")\n",
    "            ans = evaluate(tree, dict(a=1, b=1))\n",
    "            if ans != 1:\n",
    "                print(\"ID3 test 3-2 failed.\")\n",
    "            else:\n",
    "                print(\"ID3 test 3-2 succeeded.\")\n",
    "        else:\n",
    "            print(\"ID3 test 3 failed -- no tree returned\")\n",
    "    except Exception:\n",
    "        print(\"ID3 test 3 failed runtime error\")\n",
    "\n",
    "    data = [dict(a=1, b=0, c='?', Class=1), dict(a=1, b=3, c=2, Class=1),\n",
    "            dict(a=2, b='?', c=1, Class=2), dict(a=2, b=1, c=3, Class=2),\n",
    "            dict(a=3, b=0, c=1, Class=3), dict(a=3, b=2, c='?', Class=3)]\n",
    "\n",
    "    try:\n",
    "        tree = ID3(data, 0)\n",
    "        if tree != None:\n",
    "            ans = evaluate(tree, dict(a=1, b=1, c=1))\n",
    "            if ans != 1:\n",
    "                print(\"ID3 test 4-1 failed.\")\n",
    "            else:\n",
    "                print(\"ID3 test 4-1 succeeded.\")\n",
    "            ans = evaluate(tree, dict(a=2, b=0, c=0))\n",
    "            if ans != 2:\n",
    "                print(\"ID3 test 4-2 failed.\")\n",
    "            else:\n",
    "                print(\"ID3 test 4-2 succeeded.\")\n",
    "        else:\n",
    "            print(\"ID3 test 4 failed -- no tree returned\")\n",
    "    except Exception:\n",
    "        print(\"ID3 test 4 failed runtime error\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    testPruningOnHouseData('googleplaystore.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Results</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training accuracy:  0.9988137603795967\n",
    "<br>test accuracy:  0.9787234042553191"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Future works</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our future work. First, the data preprocessing needs much more effort. Right now, there are some examples in the same attribute with different value but have same meaning, we need to deal with this case first. Meanwhile, we must make sure that every type of attribute is number but not string, the numeralization of the attributes is needed here, one-hot encoding may used here to handle this problem.\n",
    "\n",
    "Then, though our decision tree gets a good result, but we don’t think that is perfect. Obviouly, some attributes like “Last Updated” might be useless. We will try to narrow down the attributes we used. After that, we will test our examples with several different method and different partition of data, try to understand why some of them are good, but others are not. In the end, we will try to come up with a criteria which take the most influential attributes into consideration for the app developers for their better development. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
